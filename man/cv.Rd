% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cross_validation.R
\name{cv}
\alias{cv}
\alias{cv.clust_analysis}
\alias{cv.min_analysis}
\alias{cv.combi_analysis}
\title{Cross-validate the clustering analysis object.}
\usage{
cv(x, ...)

\method{cv}{clust_analysis}(
  x,
  nfolds = 5,
  type = c("propagation", "som"),
  kNN = 5,
  active_variables = FALSE,
  simple_vote = TRUE,
  resolve_ties = FALSE,
  kernel_fun = function(x) 1/x,
  kNN_data = 5,
  kNN_cluster = NULL,
  seed = 1234,
  .parallel = FALSE,
  ...
)

\method{cv}{min_analysis}(x, ...)

\method{cv}{combi_analysis}(
  x,
  nfolds = 5,
  type = c("propagation", "som"),
  kNN = 5,
  active_variables = FALSE,
  simple_vote = TRUE,
  resolve_ties = FALSE,
  kernel_fun = function(x) 1/x,
  kNN_data = 5,
  kNN_cluster = NULL,
  seed = 1234,
  .parallel = FALSE,
  ...
)
}
\arguments{
\item{x}{an object.}

\item{...}{extra arguments, currently none.}

\item{nfolds}{number of CV folds.}

\item{type}{type of the prediction algorithm: k-nearest neighbors
(propagation) or via the self-organizing map ('som', available only
for SOM and combined SOM clustering).}

\item{kNN}{number of the nearest neighbors.}

\item{active_variables}{logical, should only the active variables be used for
predictions of the cluster assignment with the k-NN classifier? Applies only
to analyses made with hard threshold regularization and ignored otherwise.}

\item{simple_vote}{logical, should classical unweighted k-NN classification
be applied? If FALSE, distance-weighted k-NN is used with the provided kernel
function.}

\item{resolve_ties}{logical, should the ties be resolved at random? Applies
only to the simple unweighted voting algorithm.}

\item{kernel_fun}{kernel function transforming the distance into weight.}

\item{kNN_data}{number of the nearest neighbors in the genuine data set
used for calculation of neighborhood preservation. See \code{\link{np}}
for details.}

\item{kNN_cluster}{number of the nearest neighbors of the given cluster used
for calculation of neighborhood preservation. See \code{\link{np}} for
details.}

\item{seed}{initial setting of the random number generator.}

\item{.parallel}{logical, should the CV be run in parallel?}
}
\value{
a list of class \code{cluster_cv} containing the following elements:
\itemize{
\item the global \code{\link{clust_analysis}} object (\code{clust_analysis_object})
\item kNN projection (prediction) results (\code{predictions})
\item a data frame with the classification error, accuracy, fraction of
explained clustering variance, silhouette and neighbor preservation for
the out-of-fold predictions (\code{fold_stats})
\item means and BCA's 95\% confidence intervals for the classification error,
accuracy, fraction of explained variance, silhouette and neighborhood
preservation (\code{summary})
}

Note the \code{\link{summary.cluster_cv}} and
\code{\link{extract.cluster_cv}} methods.
}
\description{
Checks the quality of a clustering solution by
cross-validation (CV) with k-nearest neighbors (kNN) out-of-fold predictions
or predictions made by a self-organizing map (SOM).
Stability of the clustering structure is measured by cluster assignment
classification error in the out-of-fold predictions as compared with the
genuine clustering structure. Explanatory value and cluster separation are
determined by clustering variance and silhouette statistics.
}
\details{
\code{cv()} is a S3 generic function.
By principle, cross-validation of a clustering structure is similar to
cross-validation of any machine learning multi-class classifier.
The training portion of a CV split is used to develop
of a cluster structure and the projection on the test portion is accomplished
by k-nearest neighbor (kNN) label propagation algorithm or derives the
cluster assignment from a trained SOM.
For implementation details, see: \code{\link{propagate}},
\code{\link{map_som}} and \code{\link{map_supersom}}.
The folds are generated with \code{\link[caret]{createFolds}}.
For \code{combi_analysis} objects, assignment of the observations to the CV folds
is done with the kNN algorithm for the 'top' assignment of the observations
to the clusters: nodes are ignored!
For \code{clust_analysis} and \code{combi_analysis} objects with multi-layered data
and clustering of U matrix, the SOM prediction method is the sole option.
Currently, it is not possible to cross-validate clustering analysis objects
generated with an user-provided dissimilarity matrices
(subclass \code{min_analysis} of \code{clust_analysis}).
}
\references{
Lange T, Roth V, Braun ML, Buhmann JM. Stability-based validation of
clustering solutions. Neural Comput (2004) 16:1299–1323.
doi:10.1162/089976604773717621

Leng M, Wang J, Cheng J, Zhou H, Chen X. Adaptive semi-supervised
clustering algorithm with label propagation. J Softw Eng (2014) 8:14–22.
doi:10.3923/jse.2014.14.22

Kuhn M. Building predictive models in R using the caret package.
J Stat Softw (2008) 28:1–26. doi:10.18637/jss.v028.i05

Rousseeuw PJ. Silhouettes: A graphical aid to the interpretation and
validation of cluster analysis. J Comput Appl Math (1987) 20:53–65.
doi:10.1016/0377-0427(87)90125-7

Kohonen T. Self-Organizing Maps. Berlin, Heidelberg: Springer Berlin
Heidelberg (1995). doi:10.1007/978-3-642-97610-0

Wehrens R, Kruisselbrink J. Flexible self-organizing maps in kohonen 3.0.
J Stat Softw (2018) 87:1–18. doi:10.18637/jss.v087.i07

Venna J, Kaski S. Neighborhood preservation in nonlinear projection
methods: An experimental study. Lect Notes Comput Sci (including Subser Lect
Notes Artif Intell Lect Notes Bioinformatics) (2001) 2130:485–491.
doi:10.1007/3-540-44668-0_68
}
